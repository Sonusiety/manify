{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embedders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phil/miniforge3/envs/embedders2/lib/python3.10/site-packages/torch/distributions/wishart.py:271: UserWarning: Singular sample detected.\n",
      "  warnings.warn(\"Singular sample detected.\")\n"
     ]
    }
   ],
   "source": [
    "import embedders.gaussian_mixture\n",
    "\n",
    "\n",
    "pm = embedders.manifolds.ProductManifold(signature=[(1, 2), (-1, 2)])\n",
    "X, y = embedders.gaussian_mixture.gaussian_mixture(pm, num_clusters=32, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=6, out_features=6, bias=True)\n",
      "  (fc2): Linear(in_features=6, out_features=6, bias=True)\n",
      "  (fc3): Linear(in_features=6, out_features=8, bias=True)\n",
      ")\n",
      "2.0826311111450195\n",
      "1.646939992904663\n",
      "1.5910574197769165\n",
      "1.563310146331787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/0ybgtq694jnd4mbjw_0rm6dh0000gp/T/ipykernel_77890/2034484184.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = net(torch.tensor(X_train).float())\n",
      "/var/folders/ck/0ybgtq694jnd4mbjw_0rm6dh0000gp/T/ipykernel_77890/2034484184.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(y_pred, torch.tensor(y_train).long())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5462636947631836\n",
      "1.5382691621780396\n",
      "1.5338881015777588\n",
      "1.5291764736175537\n",
      "1.5273035764694214\n",
      "1.5245232582092285\n",
      "NN acc: 0.3199999928474426\n",
      "DT acc: 0.38999998569488525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/0ybgtq694jnd4mbjw_0rm6dh0000gp/T/ipykernel_77890/2034484184.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = net(torch.tensor(X_test).float())\n",
      "/var/folders/ck/0ybgtq694jnd4mbjw_0rm6dh0000gp/T/ipykernel_77890/2034484184.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"NN acc:\", (y_pred.argmax(1) == torch.tensor(y_test)).float().mean().item())\n"
     ]
    }
   ],
   "source": [
    "# Classify with basic NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import embedders.tree_new\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, pm):\n",
    "        super(Net, self).__init__()\n",
    "        self.pm = pm\n",
    "        self.fc1 = nn.Linear(6, 6)\n",
    "        self.fc2 = nn.Linear(6, 6)\n",
    "        self.fc3 = nn.Linear(6, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.relu(self.fc1(x))\n",
    "        # x = self.fc2(x)\n",
    "        # return x\n",
    "        x = self.pm.logmap(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x = self.pm.expmap(x)\n",
    "        # x = self.pm.logmap(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(pm)\n",
    "print(net)\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "for i in range(1_000):\n",
    "    opt.zero_grad()\n",
    "    y_pred = net(torch.tensor(X_train).float())\n",
    "    loss = loss_fn(y_pred, torch.tensor(y_train).long())\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if i % 100 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "y_pred = net(torch.tensor(X_test).float())\n",
    "print(\"NN acc:\", (y_pred.argmax(1) == torch.tensor(y_test)).float().mean().item())\n",
    "\n",
    "pdt = embedders.tree_new.ProductSpaceDT(pm)\n",
    "pdt.fit(X_train, y_train)\n",
    "print(\"DT acc:\", (pdt.predict(X_test) == y_test).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import embedders\n",
    "\n",
    "pm = embedders.manifolds.ProductManifold(signature=[(1, 2), (-1, 2)])\n",
    "pm.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.ambient_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: nan\n",
      "Epoch 100, Loss: nan\n",
      "Epoch 200, Loss: nan\n",
      "Epoch 300, Loss: nan\n",
      "Epoch 400, Loss: nan\n",
      "Epoch 500, Loss: nan\n",
      "Epoch 600, Loss: nan\n",
      "Epoch 700, Loss: nan\n",
      "Epoch 800, Loss: nan\n",
      "Epoch 900, Loss: nan\n",
      "TMC acc: 0.14499999582767487\n"
     ]
    }
   ],
   "source": [
    "from embedders.neural import TangentMLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pm = embedders.manifolds.ProductManifold(signature=[(1, 2), (-1, 2)])\n",
    "X, y = embedders.gaussian_mixture.gaussian_mixture(pm, num_clusters=32, num_classes=8)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "tmc = TangentMLPClassifier(pm, input_dim=6, hidden_dims=[6, 6], lr=0.01)\n",
    "tmc.fit(X_train, y_train)\n",
    "print(\"TMC acc:\", (tmc.predict(X_test) == y_test).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/0ybgtq694jnd4mbjw_0rm6dh0000gp/T/ipykernel_95564/1859281109.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = net(torch.tensor(X_train_tangent).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.171628475189209\n",
      "1.4711434841156006\n",
      "1.4593839645385742\n",
      "1.4583150148391724\n",
      "1.4568194150924683\n",
      "1.4565598964691162\n",
      "1.4560192823410034\n",
      "1.4564324617385864\n",
      "1.4585633277893066\n",
      "1.456484317779541\n",
      "NN acc: 0.3449999988079071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/0ybgtq694jnd4mbjw_0rm6dh0000gp/T/ipykernel_95564/1859281109.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = net(torch.tensor(X_test_tangent).float())\n"
     ]
    }
   ],
   "source": [
    "# super basic tangent plane mlp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X_train_tangent = pm.logmap(X_train)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(6, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 8),\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(10_000):\n",
    "    opt.zero_grad()\n",
    "    y_pred = net(torch.tensor(X_train_tangent).float())\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if i % 1_000 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "X_test_tangent = pm.logmap(X_test)\n",
    "y_pred = net(torch.tensor(X_test_tangent).float())\n",
    "print(\"NN acc:\", (y_pred.argmax(1) == y_test).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/0ybgtq694jnd4mbjw_0rm6dh0000gp/T/ipykernel_95564/3488810431.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = net(torch.tensor(X_train).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2725815773010254\n",
      "1.7822155952453613\n",
      "1.7822155952453613\n",
      "1.7822155952453613\n",
      "1.7822155952453613\n",
      "1.7822157144546509\n",
      "1.7822155952453613\n",
      "1.7822155952453613\n",
      "1.7822157144546509\n",
      "1.7822158336639404\n",
      "NN acc: 0.3700000047683716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ck/0ybgtq694jnd4mbjw_0rm6dh0000gp/T/ipykernel_95564/3488810431.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred = net(torch.tensor(X_test).float())\n"
     ]
    }
   ],
   "source": [
    "# Tangent plane MLP with no bias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(6, 8, bias=False),\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(10_000):\n",
    "    opt.zero_grad()\n",
    "    y_pred = net(torch.tensor(X_train).float())\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if i % 1_000 == 0:\n",
    "        print(loss.item())\n",
    "\n",
    "y_pred = net(torch.tensor(X_test).float())\n",
    "print(\"NN acc:\", (y_pred.argmax(1) == y_test).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc7ca0f661b480b9978c2feb5b88958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.1700\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Simple GNN model\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = torch.relu(self.conv2(x, edge_index, edge_weight))\n",
    "        return self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "\n",
    "# Create edges for a subset of nodes\n",
    "def get_subset_edges(dist_matrix, node_indices):\n",
    "    # Get submatrix of distances\n",
    "    sub_dist = dist_matrix[node_indices][:, node_indices]\n",
    "\n",
    "    # Create edges based on threshold\n",
    "    threshold = sub_dist.mean()\n",
    "    edges = (sub_dist < threshold).nonzero().t()\n",
    "\n",
    "    return edges\n",
    "\n",
    "\n",
    "def get_dense_edges(dist_matrix, node_indices):\n",
    "    # Get submatrix of distances\n",
    "    sub_dist = dist_matrix[node_indices][:, node_indices]\n",
    "\n",
    "    # Create dense edges (all-to-all connections)\n",
    "    n = len(node_indices)\n",
    "    rows = torch.arange(n).repeat_interleave(n)\n",
    "    cols = torch.arange(n).repeat(n)\n",
    "    edge_index = torch.stack([rows, cols])\n",
    "\n",
    "    # Get corresponding distances as edge weights\n",
    "    edge_weights = sub_dist.flatten()\n",
    "\n",
    "    # Convert distances to weights (you can modify this function)\n",
    "    edge_weights = torch.exp(-edge_weights)  # Gaussian kernel\n",
    "    # Alternative weightings:\n",
    "    # edge_weights = 1 / (edge_weights + 1e-6)  # Inverse distance\n",
    "    # edge_weights = torch.softmax(-edge_weights, dim=0)  # Softmax of negative distances\n",
    "\n",
    "    return edge_index, edge_weights\n",
    "\n",
    "\n",
    "# Setup\n",
    "dist_matrix = pm.pdist(X).detach()\n",
    "X_tangent = pm.logmap(X).detach()\n",
    "train_idx, test_idx = train_test_split(np.arange(len(X)), test_size=0.2)\n",
    "\n",
    "# Model, optimizer, loss\n",
    "model = SimpleGNN(in_channels=6, hidden_channels=6, out_channels=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get edges for training set\n",
    "# train_edges = get_subset_edges(dist_matrix, train_idx)\n",
    "train_edges, train_weights = get_dense_edges(dist_matrix, train_idx)\n",
    "\n",
    "# Move to Mac\n",
    "model = model.to(\"mps\")\n",
    "X_tangent = X_tangent.to(\"mps\")\n",
    "y = y.to(\"mps\")\n",
    "train_edges = train_edges.to(\"mps\")\n",
    "train_weights = train_weights.to(\"mps\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "my_tqdm = tqdm(range(10_000))\n",
    "for i in my_tqdm:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Only use training data\n",
    "    X_train = X_tangent[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "\n",
    "    # y_pred = model(X_train, train_edges)\n",
    "    y_pred = model(X_train, train_edges, train_weights)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        my_tqdm.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get edges for test set\n",
    "    # test_edges = get_subset_edges(dist_matrix, test_idx)\n",
    "    test_edges, test_weights = get_dense_edges(dist_matrix, test_idx)\n",
    "\n",
    "    test_edges = test_edges.to(\"mps\")\n",
    "    test_weights = test_weights.to(\"mps\")\n",
    "\n",
    "    # Make predictions on test set\n",
    "    X_test = X_tangent[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    # y_pred = model(X_test, test_edges)\n",
    "    y_pred = model(X_test_tangent, test_edges, test_weights)\n",
    "    acc = (y_pred.argmax(1) == y_test).float().mean().item()\n",
    "print(f\"Test accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid-based model\n",
    "import geoopt\n",
    "\n",
    "N_CLASSES = 8\n",
    "centroids = torch.vstack([pm.sample() for _ in range(N_CLASSES)])\n",
    "\n",
    "# make centroids a manifold parameter\n",
    "centroids = geoopt.ManifoldParameter(centroids, manifold=pm)\n",
    "\n",
    "# Define model: take distance to centroids as logits\n",
    "class CentroidMLR(nn.Module):\n",
    "    def __init__(self, pm, centroids):\n",
    "        super().__init__()\n",
    "        self.pm = pm\n",
    "        self.centroids = centroids\n",
    "        self.weights = nn.Parameter(torch.randn(N_CLASSES, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # p(y | h) = softmax(W h)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phil/miniforge3/envs/embedders2/lib/python3.10/site-packages/torch/distributions/wishart.py:271: UserWarning: Singular sample detected.\n",
      "  warnings.warn(\"Singular sample detected.\")\n",
      "/Users/phil/miniforge3/envs/embedders2/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/Users/phil/miniforge3/envs/embedders2/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_dt</th>\n",
       "      <th>tangent_mlp</th>\n",
       "      <th>ambient_mlp</th>\n",
       "      <th>tangent_gnn</th>\n",
       "      <th>ambient_gnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-micro</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.024543</td>\n",
       "      <td>0.233128</td>\n",
       "      <td>0.255595</td>\n",
       "      <td>25.343984</td>\n",
       "      <td>23.289283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          product_dt  tangent_mlp  ambient_mlp  tangent_gnn  ambient_gnn\n",
       "accuracy    0.300000     0.275000     0.265000     0.220000     0.220000\n",
       "f1-micro    0.300000     0.275000     0.265000     0.220000     0.220000\n",
       "time        0.024543     0.233128     0.255595    25.343984    23.289283"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Verify tangent MLP works\n",
    "import embedders\n",
    "import pandas as pd\n",
    "\n",
    "pm = embedders.manifolds.ProductManifold(signature=[(1, 2), (-1, 2)])\n",
    "X, y = embedders.gaussian_mixture.gaussian_mixture(pm, num_clusters=32, num_classes=8)\n",
    "scores = embedders.benchmarks.benchmark(\n",
    "    X, y, pm, models=[\"product_dt\", \"tangent_mlp\", \"ambient_mlp\", \"tangent_gnn\", \"ambient_gnn\"]\n",
    ")\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedders2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
