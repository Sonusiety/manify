{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sklearn_dt</th>\n",
       "      <th>sklearn_rf</th>\n",
       "      <th>product_dt</th>\n",
       "      <th>product_rf</th>\n",
       "      <th>tangent_dt</th>\n",
       "      <th>tangent_rf</th>\n",
       "      <th>knn</th>\n",
       "      <th>ps_perceptron</th>\n",
       "      <th>task</th>\n",
       "      <th>signature</th>\n",
       "      <th>dataset</th>\n",
       "      <th>table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3450</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.4050</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2,-4.0}$</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>Synthetic (single $K$)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2,-4.0}$</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>Synthetic (single $K$)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.3850</td>\n",
       "      <td>0.3800</td>\n",
       "      <td>0.3550</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2,-4.0}$</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>Synthetic (single $K$)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4150</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5400</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2,-4.0}$</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>Synthetic (single $K$)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.3550</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2,-4.0}$</td>\n",
       "      <td>Gaussian</td>\n",
       "      <td>Synthetic (single $K$)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.2674</td>\n",
       "      <td>0.3038</td>\n",
       "      <td>0.3757</td>\n",
       "      <td>0.4535</td>\n",
       "      <td>0.2594</td>\n",
       "      <td>0.3489</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.1105</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2}\\E{2}\\H{2}$</td>\n",
       "      <td>MNIST</td>\n",
       "      <td>VAE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.2773</td>\n",
       "      <td>0.3439</td>\n",
       "      <td>0.2707</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>0.2786</td>\n",
       "      <td>0.3358</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.0944</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2}\\E{2}\\H{2}$</td>\n",
       "      <td>MNIST</td>\n",
       "      <td>VAE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.2849</td>\n",
       "      <td>0.3341</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.3621</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0.3378</td>\n",
       "      <td>0.3607</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2}\\E{2}\\H{2}$</td>\n",
       "      <td>MNIST</td>\n",
       "      <td>VAE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.2589</td>\n",
       "      <td>0.3246</td>\n",
       "      <td>0.2502</td>\n",
       "      <td>0.3181</td>\n",
       "      <td>0.2594</td>\n",
       "      <td>0.3267</td>\n",
       "      <td>0.3379</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2}\\E{2}\\H{2}$</td>\n",
       "      <td>MNIST</td>\n",
       "      <td>VAE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.2659</td>\n",
       "      <td>0.3112</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.3513</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>0.3181</td>\n",
       "      <td>0.3561</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>C</td>\n",
       "      <td>$\\S{2}\\E{2}\\H{2}$</td>\n",
       "      <td>MNIST</td>\n",
       "      <td>VAE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4719 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sklearn_dt  sklearn_rf  product_dt  product_rf  tangent_dt  tangent_rf  \\\n",
       "0       0.3450      0.3150      0.3750      0.4050      0.3450      0.3450   \n",
       "1       0.3350      0.3500      0.3200      0.3500      0.3700      0.3750   \n",
       "2       0.3050      0.3000      0.3850      0.3800      0.3550      0.3900   \n",
       "3       0.4150      0.4400      0.5150      0.5200      0.4650      0.5000   \n",
       "4       0.3300      0.3450      0.3400      0.4000      0.3300      0.4100   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "35      0.2674      0.3038      0.3757      0.4535      0.2594      0.3489   \n",
       "36      0.2773      0.3439      0.2707      0.3870      0.2786      0.3358   \n",
       "37      0.2849      0.3341      0.2991      0.3621      0.2960      0.3378   \n",
       "38      0.2589      0.3246      0.2502      0.3181      0.2594      0.3267   \n",
       "39      0.2659      0.3112      0.3183      0.3513      0.2698      0.3181   \n",
       "\n",
       "       knn  ps_perceptron task          signature   dataset  \\\n",
       "0   0.4250         0.1650    C       $\\S{2,-4.0}$  Gaussian   \n",
       "1   0.3900         0.1450    C       $\\S{2,-4.0}$  Gaussian   \n",
       "2   0.4350         0.1900    C       $\\S{2,-4.0}$  Gaussian   \n",
       "3   0.5400         0.1150    C       $\\S{2,-4.0}$  Gaussian   \n",
       "4   0.3550         0.0700    C       $\\S{2,-4.0}$  Gaussian   \n",
       "..     ...            ...  ...                ...       ...   \n",
       "35  0.4671         0.1105    C  $\\S{2}\\E{2}\\H{2}$     MNIST   \n",
       "36  0.3648         0.0944    C  $\\S{2}\\E{2}\\H{2}$     MNIST   \n",
       "37  0.3607         0.1152    C  $\\S{2}\\E{2}\\H{2}$     MNIST   \n",
       "38  0.3379         0.1394    C  $\\S{2}\\E{2}\\H{2}$     MNIST   \n",
       "39  0.3561         0.1833    C  $\\S{2}\\E{2}\\H{2}$     MNIST   \n",
       "\n",
       "                     table  \n",
       "0   Synthetic (single $K$)  \n",
       "1   Synthetic (single $K$)  \n",
       "2   Synthetic (single $K$)  \n",
       "3   Synthetic (single $K$)  \n",
       "4   Synthetic (single $K$)  \n",
       "..                     ...  \n",
       "35                     VAE  \n",
       "36                     VAE  \n",
       "37                     VAE  \n",
       "38                     VAE  \n",
       "39                     VAE  \n",
       "\n",
       "[4719 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Make table\n",
    "def process_table(table, groupby, task):\n",
    "    means = table.groupby(groupby).mean()\n",
    "    stderrs = table.groupby(groupby).sem()\n",
    "\n",
    "\n",
    "# Single-signature Gaussians\n",
    "def curv2sig(curv):\n",
    "    if curv > 0:\n",
    "        return \"$\\\\H{2,\" + str(curv) + \"}$\"\n",
    "    elif curv < 0:\n",
    "        return \"$\\\\S{2,\" + str(curv) + \"}$\"\n",
    "    else:\n",
    "        return \"$\\\\E{2}$\"\n",
    "\n",
    "\n",
    "class_1man = pd.read_table(\"../data/results/classification_single_curvature.tsv\")\n",
    "class_1man[\"task\"] = \"C\"\n",
    "class_1man[\"signature\"] = class_1man[\"curvature\"].map(curv2sig)\n",
    "class_1man[\"dataset\"] = \"Gaussian\"\n",
    "\n",
    "reg_1man = pd.read_table(\"../data/results/regression_single_curvature.tsv\")\n",
    "reg_1man[\"task\"] = \"R\"\n",
    "reg_1man[\"signature\"] = reg_1man[\"curvature\"].map(curv2sig)\n",
    "reg_1man[\"dataset\"] = \"Gaussian\"\n",
    "\n",
    "# Multiple-signature Gaussians\n",
    "sig_dict = {\n",
    "    \"H\": \"$\\\\H{4}$\",\n",
    "    \"E\": \"$\\\\E{4}$\",\n",
    "    \"S\": \"$\\\\S{4}$\",\n",
    "    \"HH\": \"($\\\\H{2})^2$\",\n",
    "    \"HE\": \"$\\\\H{2}\\\\E{2}$\",\n",
    "    \"HS\": \"$\\\\H{2}\\\\S{2}$\",\n",
    "    \"SE\": \"$\\\\S{2}\\\\E{2}$\",\n",
    "    \"SS\": \"$(\\\\S{2})^2$\",\n",
    "}\n",
    "\n",
    "class_sig = pd.read_table(\"../data/results/classification_signature.tsv\")\n",
    "class_sig[\"task\"] = \"C\"\n",
    "class_sig[\"signature\"] = class_sig[\"signature\"].map(sig_dict)\n",
    "class_sig[\"dataset\"] = \"Gaussian\"\n",
    "\n",
    "reg_sig = pd.read_table(\"../data/results/regression_signature.tsv\")\n",
    "reg_sig[\"task\"] = \"R\"\n",
    "reg_sig[\"signature\"] = reg_sig[\"signature\"].map(sig_dict)\n",
    "reg_sig[\"dataset\"] = \"Gaussian\"\n",
    "\n",
    "# Empirical datasets\n",
    "temp = pd.read_table(\"../data/results/temperature.tsv\")\n",
    "temp[\"task\"] = \"R\"\n",
    "temp[\"signature\"] = \"$\\\\S{2}\\\\S{1}$\"\n",
    "temp[\"dataset\"] = \"Temperature\"\n",
    "\n",
    "fourier = pd.read_table(\"../data/results/fourier.tsv\")\n",
    "fourier[\"task\"] = \"C\"\n",
    "fourier[\"signature\"] = \"$(\\\\S{1})^5$\"\n",
    "fourier[\"dataset\"] = \"Neuron 33\"\n",
    "\n",
    "fourier2 = pd.read_table(\"../data/results/fourier2.tsv\")\n",
    "fourier2[\"task\"] = \"C\"\n",
    "fourier2[\"signature\"] = \"$(\\\\S{1})^5$\"\n",
    "fourier2[\"dataset\"] = \"Neuron 46\"\n",
    "\n",
    "traffic = pd.read_table(\"../data/results/traffic_results.tsv\")\n",
    "traffic[\"task\"] = \"R\"\n",
    "traffic[\"signature\"] = \"$\\\\E{1}(\\\\S{1})^4$\"\n",
    "traffic[\"dataset\"] = \"Traffic\"\n",
    "\n",
    "land_water = pd.read_table(\"../data/results/land_vs_water.tsv\")\n",
    "land_water[\"task\"] = \"C\"\n",
    "land_water[\"signature\"] = \"$\\\\S{2}$\"\n",
    "land_water[\"dataset\"] = \"Landmasses\"\n",
    "\n",
    "# Graph datasets - only keep lowest d_avg\n",
    "graph_task_dict = {\n",
    "    \"polblogs\": \"C\",\n",
    "    \"citeseer\": \"C\",\n",
    "    \"cora\": \"C\",\n",
    "    \"cs_phds\": \"R\",\n",
    "}\n",
    "graph_dataset_names = {\n",
    "    \"polblogs\": \"PolBlogs\",\n",
    "    \"citeseer\": \"CiteSeer\",\n",
    "    \"cora\": \"Cora\",\n",
    "    \"cs_phds\": \"CS PhDs\",\n",
    "}\n",
    "graphs = pd.read_table(\"../data/results/graphs.tsv\")\n",
    "# graphs = graphs.groupby([\"embedding\", \"signature\"]).mean().sort_values(\"d_avg\").reset_index().groupby(\"embedding\").first().reset_index()\n",
    "# best_sigs = (\n",
    "#     graphs.groupby([\"embedding\", \"signature\"])\n",
    "#     .mean()\n",
    "#     .sort_values(\"d_avg\")\n",
    "#     .reset_index()\n",
    "#     .groupby(\"embedding\")\n",
    "#     .first()\n",
    "#     .reset_index()[[\"embedding\", \"signature\"]]\n",
    "# )\n",
    "# graphs = pd.merge(graphs, best_sigs, on=[\"embedding\", \"signature\"])\n",
    "graphs[\"signature\"] = graphs[\"signature\"].map(sig_dict)\n",
    "graphs[\"task\"] = graphs[\"embedding\"].map(graph_task_dict)\n",
    "graphs[\"dataset\"] = graphs[\"embedding\"].map(graph_dataset_names)\n",
    "\n",
    "# Link prediction datasets\n",
    "link_dataset_names = {\n",
    "    \"football\": \"Football\",\n",
    "    \"karate_club\": \"Karate Club\",\n",
    "    \"polbooks\": \"PolBooks\",\n",
    "    \"adjnoun\": \"AdjNoun\",\n",
    "    \"dolphins\": \"Dolphins\",\n",
    "    \"lesmis\": \"Les Mis\",\n",
    "}\n",
    "links = pd.read_table(\"../data/results/link_prediction.tsv\")\n",
    "links[\"task\"] = \"LP\"\n",
    "# links[\"signature\"] = \"$(\\\\S{2}\\\\E{2}\\\\H{2})^2\\\\E{1}$\"\n",
    "links[\"signature\"] = \"$\\\\S{2}\\\\E{2}\\\\H{2}$\"\n",
    "links[\"dataset\"] = links[\"dataset\"].map(link_dataset_names)\n",
    "\n",
    "# VAE dataset\n",
    "vae_signature_dict = {\n",
    "    \"blood_cell_scrna\": \"$\\\\S{2}\\\\E{2}(\\\\H{2})^3$\",\n",
    "    \"lymphoma\": \"$(\\\\S{2})^2$\",\n",
    "    \"cifar_100\": \"$(\\\\S{2})^4$\",\n",
    "    \"mnist\": \"$\\\\S{2}\\\\E{2}\\\\H{2}$\",\n",
    "}\n",
    "vae_dataset_names = {\n",
    "    \"blood_cell_scrna\": \"Blood\",\n",
    "    \"lymphoma\": \"Lymphoma\",\n",
    "    \"cifar_100\": \"CIFAR-100\",\n",
    "    \"mnist\": \"MNIST\",\n",
    "}\n",
    "vae = pd.read_table(\"../data/results/vae.tsv\")\n",
    "vae[\"task\"] = \"C\"\n",
    "vae[\"signature\"] = vae[\"embedding\"].map(vae_signature_dict)\n",
    "vae[\"dataset\"] = vae[\"embedding\"].map(vae_dataset_names)\n",
    "\n",
    "# Put it all together\n",
    "class_1man[\"table\"] = \"Synthetic (single $K$)\"\n",
    "reg_1man[\"table\"] = \"Synthetic (single $K$)\"\n",
    "class_sig[\"table\"] = \"Synthetic (multi-$K$)\"\n",
    "reg_sig[\"table\"] = \"Synthetic (multi-$K$)\"\n",
    "# temp[\"table\"] = \"temperature\"\n",
    "# fourier[\"table\"] = \"fourier\"\n",
    "temp[\"table\"] = \"Other\"\n",
    "fourier[\"table\"] = \"Other\"\n",
    "fourier2[\"table\"] = \"Other\"\n",
    "traffic[\"table\"] = \"Other\"\n",
    "land_water[\"table\"] = \"Other\"\n",
    "graphs[\"table\"] = \"Graph embeddings\"\n",
    "links[\"table\"] = \"Graph embeddings\"\n",
    "vae[\"table\"] = \"VAE\"\n",
    "\n",
    "all_data = pd.concat(\n",
    "    [class_1man, reg_1man, class_sig, reg_sig, temp, fourier, fourier2, traffic, land_water, graphs, links, vae]\n",
    ")\n",
    "all_data = all_data.drop(columns=[\"embedding\", \"curvature\", \"d_avg\", \"seed\", \"trial\"])\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv(\"../data/results/all_results.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LaTeX table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_table(\"../data/results/all_results.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "import pandas as pd\n",
    "\n",
    "USE_BONFERRONI = True\n",
    "P_VAL = 0.05\n",
    "ADJUSTMENT = 10 if USE_BONFERRONI else 1  # 5 choose 2\n",
    "\n",
    "\n",
    "# Function to calculate mean and standard error\n",
    "def mean_se(series):\n",
    "    return series.mean(), series.sem(), series.count()\n",
    "\n",
    "\n",
    "# Define predictors to compare\n",
    "group1 = [\"product_dt\", \"sklearn_dt\", \"tangent_dt\", \"knn\", \"ps_perceptron\"]\n",
    "group2 = [\"product_rf\", \"sklearn_rf\", \"tangent_rf\", \"knn\", \"ps_perceptron\"]\n",
    "\n",
    "# Initialize LaTeX symbols for Wilcoxon test results\n",
    "latex_symbols = {\n",
    "    \"product_dt\": \"\\col{product_dt}{*}\",\n",
    "    \"product_rf\": \"\\col{product_dt}{*}\",\n",
    "    \"sklearn_dt\": \"\\col{euclidean_dt}{â€ }\",\n",
    "    \"sklearn_rf\": \"\\col{euclidean_dt}{â€ }\",\n",
    "    \"tangent_dt\": \"\\col{tangent_dt}{â€¡}\",\n",
    "    \"tangent_rf\": \"\\col{tangent_dt}{â€¡}\",\n",
    "    \"knn\": \"\\col{knn}{Â§}\",\n",
    "    \"ps_perceptron\": \"\\col{perceptron}{Â¶}\",\n",
    "}\n",
    "\n",
    "\n",
    "# Function to format a LaTeX cell with mean Â± SE and symbols\n",
    "def format_latex_cell(mean, se, bold=False, underline=False, symbols=None, regression=False):\n",
    "    if regression:\n",
    "        # formatted = f\"{mean:.3f}\".lstrip(\"0\") + \" \\scriptsize Â± \" + f\"{se:.3f}\".lstrip(\"0\")\n",
    "        formatted = f\"{mean:.3f}\".lstrip(\"0\") + \" Â± \" + f\"{se:.3f}\".lstrip(\"0\")\n",
    "    else:\n",
    "        # formatted = f\"{mean * 100:.1f}\".lstrip(\"0\") + \"\\scriptsize Â± \" + f\"{se * 100:.1f}\".lstrip(\"0\")\n",
    "        formatted = f\"{mean * 100:.1f}\".lstrip(\"0\") + \" Â± \" + f\"{se * 100:.1f}\".lstrip(\"0\")\n",
    "    if bold:\n",
    "        formatted = f\"\\\\textbf{{{formatted}}}\"\n",
    "    if underline:\n",
    "        formatted = f\"\\\\underline{{{formatted}}}\"\n",
    "    if symbols:\n",
    "        # Remove duplicates\n",
    "        symbols = sorted(list(set(symbols)))\n",
    "        formatted += f\"\\\\textsuperscript{{{''.join(symbols)}}}\"\n",
    "    return formatted\n",
    "\n",
    "\n",
    "# Function to run Wilcoxon test between predictors within group1 and within group2\n",
    "def run_wilcoxon_tests(group1_vals, group2_vals, regression=False):\n",
    "    symbols = {key: [] for key in group1 + group2}\n",
    "\n",
    "    # Compare all pairs within group1\n",
    "    for i, pred1 in enumerate(group1):\n",
    "        for pred2 in group1[i + 1 :]:\n",
    "            if group1_vals[pred1].isnull().all() or group1_vals[pred2].isnull().all():\n",
    "                continue\n",
    "            elif (not regression and group1_vals[pred1].mean() > group1_vals[pred2].mean()) or (\n",
    "                regression and group1_vals[pred1].mean() < group1_vals[pred2].mean()\n",
    "            ):\n",
    "                stat, p_value = wilcoxon(group1_vals[pred1], group1_vals[pred2])\n",
    "                if p_value < P_VAL / ADJUSTMENT:\n",
    "                    symbols[pred1].append(latex_symbols[pred2])\n",
    "                    symbols[pred2].append(latex_symbols[pred1])\n",
    "\n",
    "    # Compare all pairs within group2\n",
    "    for i, pred1 in enumerate(group2):\n",
    "        for pred2 in group2[i + 1 :]:\n",
    "            if group2_vals[pred1].isnull().all() or group2_vals[pred2].isnull().all():\n",
    "                continue\n",
    "            # elif group2_vals[pred1].mean() > group2_vals[pred2].mean():\n",
    "            elif (not regression and group2_vals[pred1].mean() > group2_vals[pred2].mean()) or (\n",
    "                regression and group2_vals[pred1].mean() < group2_vals[pred2].mean()\n",
    "            ):\n",
    "                stat, p_value = wilcoxon(group2_vals[pred1], group2_vals[pred2])\n",
    "                if p_value < P_VAL / ADJUSTMENT:\n",
    "                    symbols[pred1].append(latex_symbols[pred2])\n",
    "                    symbols[pred2].append(latex_symbols[pred1])\n",
    "\n",
    "    return symbols\n",
    "\n",
    "\n",
    "# Group by task and dataset\n",
    "# grouped = all_data.groupby([\"table\", \"task\", \"dataset\", \"signature\"])\n",
    "grouped = all_data.groupby([\"table\", \"dataset\", \"task\", \"signature\"])\n",
    "\n",
    "# Iterate through each group and process data\n",
    "results = {}\n",
    "# for (table, task, dataset, signature), group in grouped:\n",
    "for (table, dataset, task, signature), group in grouped:\n",
    "    group_results = {}\n",
    "    n_samples = {}\n",
    "\n",
    "    # Calculate mean and SE for each predictor\n",
    "    for pred in group1 + group2:\n",
    "        mean, se, n = mean_se(group[pred])\n",
    "        group_results[pred] = {\"mean\": mean, \"se\": se}\n",
    "        n_samples[pred] = n\n",
    "\n",
    "    # Run Wilcoxon tests and gather symbols\n",
    "    symbols = run_wilcoxon_tests(group[group1], group[group2], regression=(task == \"R\"))\n",
    "\n",
    "    # Bold the best predictor\n",
    "    if task == \"R\":\n",
    "        best_pred = min(group_results, key=lambda x: group_results[x][\"mean\"])\n",
    "        second_best_pred = sorted(group_results, key=lambda x: group_results[x][\"mean\"])[1]\n",
    "    else:  # LP and C\n",
    "        best_pred = max(group_results, key=lambda x: group_results[x][\"mean\"])\n",
    "        second_best_pred = sorted(group_results, key=lambda x: group_results[x][\"mean\"])[-2]\n",
    "\n",
    "    # Format the results for LaTeX\n",
    "    # latex_table = {\"$n$\": max(n_samples.values())}\n",
    "    latex_table = {}\n",
    "    for pred in group1 + group2:\n",
    "        bold = pred == best_pred\n",
    "        underline = pred == second_best_pred\n",
    "        latex_table[pred] = format_latex_cell(\n",
    "            group_results[pred][\"mean\"],\n",
    "            group_results[pred][\"se\"],\n",
    "            bold=bold,\n",
    "            underline=underline,\n",
    "            symbols=symbols.get(pred, []),\n",
    "            regression=(task == \"R\"),\n",
    "        )\n",
    "\n",
    "    # results[(table, task, dataset, signature)] = latex_table\n",
    "    results[(table, dataset, task, signature)] = latex_table\n",
    "\n",
    "# Convert results into a dataframe for display\n",
    "latex_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "latex_df.columns = [\n",
    "    # \"$n$\",\n",
    "    \"\\col{product_dt}{Product DT}\",\n",
    "    \"\\col{euclidean_dt}{Euclidean DT}\",\n",
    "    \"\\col{tangent_dt}{Tangent DT}\",\n",
    "    \"\\col{knn}{$k$-Neighbors}\",\n",
    "    \"\\col{perceptron}{PS Perceptron}\",\n",
    "    \"\\col{product_dt}{Product RF}\",\n",
    "    \"\\col{euclidean_dt}{Euclidean RF}\",\n",
    "    \"\\col{tangent_dt}{Tangent RF}\",\n",
    "]\n",
    "\n",
    "# How I want it sorted\n",
    "sort_order_l1 = [\n",
    "    \"Synthetic (single $K$)\",\n",
    "    \"Synthetic (multi-$K$)\",\n",
    "    # \"Empirical\",\n",
    "    \"Graph embeddings\",\n",
    "    # \"VAE embeddings\",\n",
    "    # \"Graph\",\n",
    "    \"VAE\",\n",
    "    \"Other\",\n",
    "]\n",
    "sort_order_l2 = [\"C\", \"R\", \"LP\"]\n",
    "sort_order_l3 = [\n",
    "    \"Gaussian mixture\",\n",
    "    \"Global temperature\",\n",
    "    \"Neural spiking\",\n",
    "    \"PolBlogs\",\n",
    "    \"CiteSeer\",\n",
    "    \"Cora\",\n",
    "    \"CS PhDs\",\n",
    "    \"Football\",\n",
    "    \"Karate Club\",\n",
    "    \"PolBooks\",\n",
    "    \"AdjNoun\",\n",
    "    \"Blood\",\n",
    "    \"Lymphoma\",\n",
    "    \"CIFAR-100\",\n",
    "    \"MNIST\",\n",
    "    \"Traffic\"\n",
    "]\n",
    "sort_order_l3 = [sig_dict[sig] for sig in sig_dict]\n",
    "\n",
    "# Make dicts\n",
    "sort_dict1 = {name: i for i, name in enumerate(sort_order_l1)}\n",
    "sort_dict2 = {name: i for i, name in enumerate(sort_order_l2)}\n",
    "sort_dict3 = {name: i for i, name in enumerate(sort_order_l3)}\n",
    "\n",
    "latex_df = (\n",
    "    latex_df.assign(\n",
    "        sort1=latex_df.index.get_level_values(0).map(sort_dict1),\n",
    "        # sort2=latex_df.index.get_level_values(1).map(sort_dict2),\n",
    "        sort2=latex_df.index.get_level_values(1).map(sort_dict3),\n",
    "        # sort3=latex_df.index.get_level_values(2).map(sort_dict3),\n",
    "        sort3=latex_df.index.get_level_values(2).map(sort_dict2),\n",
    "    )\n",
    "    .sort_values([\"sort1\", \"sort2\", \"sort3\"])\n",
    "    .drop(columns=[\"sort1\", \"sort2\", \"sort3\"])\n",
    ")\n",
    "\n",
    "# Rotate index labels 1 and 2; add spacing for better alignment\n",
    "c_dict = {\n",
    "    \"Synthetic (single $K$)\": \"-5.5cm\",\n",
    "    \"Synthetic (multi-$K$)\": \"-4.5cm\",\n",
    "    \"Graph embeddings\": \"-3cm\",\n",
    "    \"VAE\": \"-1cm\",\n",
    "    \"Other\": \"-1cm\",\n",
    "}\n",
    "latex_df.index = pd.MultiIndex.from_tuples(\n",
    "    # [(f\"\\\\rotatebox{{-90}}{{{c}}}\", f\"\\\\rotatebox{{-90}}{{{t}}}\", d, s) for c, t, d, s in latex_df.index],\n",
    "    [(f\"\\\\rotatebox{{90}}{{\\\\hspace{{{c_dict[c]}}}{c}}}\", t, d, s) for c, t, d, s in latex_df.index],\n",
    "    # names=[\"\", \"Task\", \"Dataset\", \"Signature\"],\n",
    "    names=[\"\", \"Dataset\", \"Task\", \"Signature\"],\n",
    ")\n",
    "\n",
    "# Drop columns that never win; rearrange to suit our needs\n",
    "# latex_df = latex_df.drop(\n",
    "    # columns=[\"\\col{tangent_dt}{Tangent DT}\", \"\\col{perceptron}{PS Perceptron}\", \"\\col{tangent_dt}{Tangent RF}\"]\n",
    "# )\n",
    "latex_df = latex_df[\n",
    "    [\n",
    "        \"\\col{perceptron}{PS Perceptron}\",\n",
    "        \"\\col{knn}{$k$-Neighbors}\",\n",
    "        \"\\col{euclidean_dt}{Euclidean DT}\",\n",
    "        \"\\col{euclidean_dt}{Euclidean RF}\",\n",
    "        \"\\col{tangent_dt}{Tangent DT}\",\n",
    "        \"\\col{tangent_dt}{Tangent RF}\",\n",
    "        \"\\col{product_dt}{Product DT}\",\n",
    "        \"\\col{product_dt}{Product RF}\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = latex_df.to_latex(\n",
    "    # \"../data/results/latex_table.tex\",\n",
    "    escape=False,\n",
    "    # column_format=\"|>{\\centering\\\\arraybackslash}p{2cm}|>{\\centering\\\\arraybackslash}p{2cm}|p{2cm}|p{2cm}|\",\n",
    "    # column_format=\">{\\centering\\\\arraybackslash}p{2cm}>{\\centering\\\\arraybackslash}p{2cm}p{2cm}p{2cm}rrrrrrrrr\",\n",
    "    # column_format=\">{\\centering\\\\arraybackslash}p{1cm}>{\\centering\\\\arraybackslash}p{1cm}p{1.5cm}p{1.5cm}llllllll\",\n",
    "    header=True,\n",
    ")\n",
    "\n",
    "# Remove all occurrences of \"\\cline{3-9}\"\n",
    "latex = latex.replace(\"\\\\cline{3-9}\", \"\")\n",
    "latex = latex.replace(\"\\\\cline{2-9}\", \"\")\n",
    "\n",
    "# Change top bar\n",
    "my_toprule = \"\"\"\\\\toprule\n",
    "& Dataset & Task & Signature & \\col{knn}{$k$-Neighbors} & \\col{euclidean_dt}{Euclidean \\\\ DT} & \\col{euclidean_dt}{Euclidean RF} & \\col{product_dt}{Product DT} & \\col{product_dt}{Product RF} \\\\\\\\\n",
    "\\midrule\"\"\"\n",
    "latex = latex.split(\"\\n\")\n",
    "latex = [latex[0], my_toprule] + latex[5:-4] + latex[-3:]\n",
    "latex = \"\\n\".join(latex)\n",
    "\n",
    "with open(\"../data/results/latex_table.tex\", \"w\") as f:\n",
    "    f.write(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "knn           22\n",
       "product_rf    16\n",
       "sklearn_dt    10\n",
       "product_dt     3\n",
       "tangent_dt     3\n",
       "sklearn_rf     2\n",
       "tangent_rf     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How often are we the max?\n",
    "\n",
    "all_data.groupby([\"table\", \"task\", \"dataset\", \"signature\"]).mean().idxmax(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_rf    30\n",
      "knn           13\n",
      "sklearn_rf     7\n",
      "sklearn_dt     4\n",
      "product_dt     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "product_rf       16\n",
      "tangent_rf       14\n",
      "sklearn_rf       12\n",
      "knn               6\n",
      "sklearn_dt        6\n",
      "product_dt        1\n",
      "ps_perceptron     1\n",
      "tangent_dt        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "True     52\n",
      "False     5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "alldata_grouped = all_data.groupby([\"table\", \"task\", \"dataset\", \"signature\"]).mean()\n",
    "\n",
    "# Flip the sign for regression tasks\n",
    "alldata_grouped.loc[alldata_grouped.index.get_level_values(\"task\") == \"R\"] = -alldata_grouped.loc[\n",
    "    alldata_grouped.index.get_level_values(\"task\") == \"R\"\n",
    "]\n",
    "\n",
    "# How often are we the max?\n",
    "print(alldata_grouped.idxmax(axis=1).value_counts())\n",
    "print()\n",
    "\n",
    "# What about second best?\n",
    "print(alldata_grouped.apply(lambda x: x.nlargest(2).idxmin(), axis=1).value_counts())\n",
    "print()\n",
    "\n",
    "# How often is at least one of our predictors in the top 2?\n",
    "\n",
    "# Then, apply the check for top 2 scores in those columns\n",
    "result = alldata_grouped.apply(\n",
    "    lambda x: any(col in x.nlargest(2).index for col in [\"product_dt\", \"product_rf\"]), axis=1\n",
    ")\n",
    "# Print the counts of True/False values\n",
    "print(result.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedders2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
