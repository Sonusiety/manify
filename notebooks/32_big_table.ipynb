{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/results_icml/regression_signature.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m class_sig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m class_sig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(sig_dict)\n\u001b[1;32m     45\u001b[0m class_sig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGaussian\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 47\u001b[0m reg_sig \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/results_icml/regression_signature.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m reg_sig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m reg_sig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m reg_sig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(sig_dict)\n",
      "File \u001b[0;32m~/miniconda3/envs/embedders/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1405\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1392\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1393\u001b[0m     dialect,\n\u001b[1;32m   1394\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1401\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1402\u001b[0m )\n\u001b[1;32m   1403\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/embedders/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/embedders/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/embedders/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/embedders/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/results_icml/regression_signature.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Make table\n",
    "def process_table(table, groupby, task):\n",
    "    means = table.groupby(groupby).mean()\n",
    "    stderrs = table.groupby(groupby).sem()\n",
    "\n",
    "\n",
    "# Single-signature Gaussians\n",
    "def curv2sig(curv):\n",
    "    if curv > 0:\n",
    "        return \"$\\\\H{2,\" + str(curv) + \"}$\"\n",
    "    elif curv < 0:\n",
    "        return \"$\\\\S{2,\" + str(curv) + \"}$\"\n",
    "    else:\n",
    "        return \"$\\\\E{2}$\"\n",
    "\n",
    "\n",
    "class_1man = pd.read_table(\"../data/results_icml/classification_single_curvature.tsv\")\n",
    "class_1man[\"task\"] = \"C\"\n",
    "class_1man[\"signature\"] = class_1man[\"curvature\"].map(curv2sig)\n",
    "class_1man[\"dataset\"] = \"Gaussian\"\n",
    "\n",
    "reg_1man = pd.read_table(\"../data/results_icml/regression_single_curvature.tsv\")\n",
    "reg_1man[\"task\"] = \"R\"\n",
    "reg_1man[\"signature\"] = reg_1man[\"curvature\"].map(curv2sig)\n",
    "reg_1man[\"dataset\"] = \"Gaussian\"\n",
    "\n",
    "# Multiple-signature Gaussians\n",
    "sig_dict = {\n",
    "    \"H\": \"$\\\\H{4}$\",\n",
    "    \"E\": \"$\\\\E{4}$\",\n",
    "    \"S\": \"$\\\\S{4}$\",\n",
    "    \"HH\": \"($\\\\H{2})^2$\",\n",
    "    \"HE\": \"$\\\\H{2}\\\\E{2}$\",\n",
    "    \"HS\": \"$\\\\H{2}\\\\S{2}$\",\n",
    "    \"SE\": \"$\\\\S{2}\\\\E{2}$\",\n",
    "    \"SS\": \"$(\\\\S{2})^2$\",\n",
    "}\n",
    "\n",
    "class_sig = pd.read_table(\"../data/results_icml/classification_signature.tsv\")\n",
    "class_sig[\"task\"] = \"C\"\n",
    "class_sig[\"signature\"] = class_sig[\"signature\"].map(sig_dict)\n",
    "class_sig[\"dataset\"] = \"Gaussian\"\n",
    "\n",
    "reg_sig = pd.read_table(\"../data/results_icml/regression_signature.tsv\")\n",
    "reg_sig[\"task\"] = \"R\"\n",
    "reg_sig[\"signature\"] = reg_sig[\"signature\"].map(sig_dict)\n",
    "reg_sig[\"dataset\"] = \"Gaussian\"\n",
    "\n",
    "# Empirical datasets\n",
    "# temp = pd.read_table(\"../data/results_icml/temperature.tsv\")\n",
    "# temp[\"task\"] = \"R\"\n",
    "# temp[\"signature\"] = \"$\\\\S{2}\\\\S{1}$\"\n",
    "# temp[\"dataset\"] = \"Temperature\"\n",
    "\n",
    "# fourier = pd.read_table(\"../data/results_icml/fourier.tsv\")\n",
    "# fourier[\"task\"] = \"C\"\n",
    "# fourier[\"signature\"] = \"$(\\\\S{1})^5$\"\n",
    "# fourier[\"dataset\"] = \"Neuron 33\"\n",
    "\n",
    "# fourier2 = pd.read_table(\"../data/results_icml/fourier2.tsv\")\n",
    "# fourier2[\"task\"] = \"C\"\n",
    "# fourier2[\"signature\"] = \"$(\\\\S{1})^5$\"\n",
    "# fourier2[\"dataset\"] = \"Neuron 46\"\n",
    "\n",
    "# traffic = pd.read_table(\"../data/results_icml/traffic_results.tsv\")\n",
    "# traffic[\"task\"] = \"R\"\n",
    "# traffic[\"signature\"] = \"$\\\\E{1}(\\\\S{1})^4$\"\n",
    "# traffic[\"dataset\"] = \"Traffic\"\n",
    "\n",
    "# land_water = pd.read_table(\"../data/results_icml/land_vs_water.tsv\")\n",
    "# land_water[\"task\"] = \"C\"\n",
    "# land_water[\"signature\"] = \"$\\\\S{2}$\"\n",
    "# land_water[\"dataset\"] = \"Landmasses\"\n",
    "\n",
    "# Graph datasets - only keep lowest d_avg\n",
    "# graph_task_dict = {\n",
    "#     \"polblogs\": \"C\",\n",
    "#     \"citeseer\": \"C\",\n",
    "#     \"cora\": \"C\",\n",
    "#     \"cs_phds\": \"R\",\n",
    "# }\n",
    "graph_dataset_names = {\n",
    "    \"polblogs\": \"PolBlogs\",\n",
    "    \"citeseer\": \"CiteSeer\",\n",
    "    \"cora\": \"Cora\",\n",
    "    \"cs_phds\": \"CS PhDs\",\n",
    "}\n",
    "graphs_c = pd.read_table(\"../data/results_icml/graph_classification.tsv\")\n",
    "# graphs = graphs.groupby([\"embedding\", \"signature\"]).mean().sort_values(\"d_avg\").reset_index().groupby(\"embedding\").first().reset_index()\n",
    "# best_sigs = (\n",
    "#     graphs.groupby([\"embedding\", \"signature\"])\n",
    "#     .mean()\n",
    "#     .sort_values(\"d_avg\")\n",
    "#     .reset_index()\n",
    "#     .groupby(\"embedding\")\n",
    "#     .first()\n",
    "#     .reset_index()[[\"embedding\", \"signature\"]]\n",
    "# )\n",
    "# graphs = pd.merge(graphs, best_sigs, on=[\"embedding\", \"signature\"])\n",
    "graphs_c[\"signature\"] = graphs[\"signature\"].map(sig_dict)\n",
    "graphs_c[\"task\"] = graphs[\"embedding\"].map(graph_task_dict)\n",
    "graphs_c[\"dataset\"] = graphs[\"embedding\"].map(graph_dataset_names)\n",
    "\n",
    "graphs_r = pd.read_table(\"../data/results_icml/graph_regression.tsv\")\n",
    "graphs_r[\"signature\"] = graphs[\"signature\"].map(sig_dict)\n",
    "graphs_r[\"task\"] = graphs[\"embedding\"].map(graph_task_dict)\n",
    "graphs_r[\"dataset\"] = graphs[\"embedding\"].map(graph_dataset_names)\n",
    "\n",
    "graphs = pd.concat([graphs_c, graphs_r])\n",
    "\n",
    "# # Link prediction datasets\n",
    "# link_dataset_names = {\n",
    "#     \"football\": \"Football\",\n",
    "#     \"karate_club\": \"Karate Club\",\n",
    "#     \"polbooks\": \"PolBooks\",\n",
    "#     \"adjnoun\": \"AdjNoun\",\n",
    "#     \"dolphins\": \"Dolphins\",\n",
    "#     \"lesmis\": \"Les Mis\",\n",
    "# }\n",
    "# links = pd.read_table(\"../data/results/link_prediction.tsv\")\n",
    "# links[\"task\"] = \"LP\"\n",
    "# # links[\"signature\"] = \"$(\\\\S{2}\\\\E{2}\\\\H{2})^2\\\\E{1}$\"\n",
    "# links[\"signature\"] = \"$\\\\S{2}\\\\E{2}\\\\H{2}$\"\n",
    "# links[\"dataset\"] = links[\"dataset\"].map(link_dataset_names)\n",
    "\n",
    "# VAE dataset\n",
    "vae_signature_dict = {\n",
    "    \"blood_cell_scrna\": \"$\\\\S{2}\\\\E{2}(\\\\H{2})^3$\",\n",
    "    \"lymphoma\": \"$(\\\\S{2})^2$\",\n",
    "    \"cifar_100\": \"$(\\\\S{2})^4$\",\n",
    "    \"mnist\": \"$\\\\S{2}\\\\E{2}\\\\H{2}$\",\n",
    "}\n",
    "vae_dataset_names = {\n",
    "    \"blood_cell_scrna\": \"Blood\",\n",
    "    \"lymphoma\": \"Lymphoma\",\n",
    "    \"cifar_100\": \"CIFAR-100\",\n",
    "    \"mnist\": \"MNIST\",\n",
    "}\n",
    "vae = pd.read_table(\"../data/results_cml/vae.tsv\")\n",
    "vae[\"task\"] = \"C\"\n",
    "vae[\"signature\"] = vae[\"embedding\"].map(vae_signature_dict)\n",
    "vae[\"dataset\"] = vae[\"embedding\"].map(vae_dataset_names)\n",
    "\n",
    "# Put it all together\n",
    "class_1man[\"table\"] = \"Synthetic (single $K$)\"\n",
    "reg_1man[\"table\"] = \"Synthetic (single $K$)\"\n",
    "class_sig[\"table\"] = \"Synthetic (multi-$K$)\"\n",
    "reg_sig[\"table\"] = \"Synthetic (multi-$K$)\"\n",
    "temp[\"table\"] = \"Other\"\n",
    "# fourier[\"table\"] = \"Other\"\n",
    "# fourier2[\"table\"] = \"Other\"\n",
    "# traffic[\"table\"] = \"Other\"\n",
    "# land_water[\"table\"] = \"Other\"\n",
    "graphs[\"table\"] = \"Graph embeddings\"\n",
    "# links[\"table\"] = \"Graph embeddings\"\n",
    "vae[\"table\"] = \"VAE\"\n",
    "\n",
    "all_data = pd.concat(\n",
    "    [\n",
    "        class_1man, \n",
    "        reg_1man, \n",
    "        class_sig, \n",
    "        reg_sig, \n",
    "        # temp, \n",
    "        # fourier, \n",
    "        # fourier2, \n",
    "        # traffic, \n",
    "        # land_water, \n",
    "        graphs, \n",
    "        # links, \n",
    "        vae\n",
    "    ]\n",
    ")\n",
    "all_data = all_data.drop(columns=[\"embedding\", \"curvature\", \"d_avg\", \"seed\", \"trial\"])\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv(\"../data/results/all_results.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LaTeX table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_table(\"../data/results/all_results.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "import pandas as pd\n",
    "\n",
    "USE_BONFERRONI = True\n",
    "P_VAL = 0.05\n",
    "ADJUSTMENT = 10 if USE_BONFERRONI else 1  # 5 choose 2\n",
    "\n",
    "\n",
    "# Function to calculate mean and standard error\n",
    "def mean_se(series):\n",
    "    return series.mean(), series.sem(), series.count()\n",
    "\n",
    "\n",
    "# Define predictors to compare\n",
    "group1 = [\"product_dt\", \"sklearn_dt\", \"tangent_dt\", \"knn\", \"ps_perceptron\"]\n",
    "group2 = [\"product_rf\", \"sklearn_rf\", \"tangent_rf\", \"knn\", \"ps_perceptron\"]\n",
    "\n",
    "# Initialize LaTeX symbols for Wilcoxon test results\n",
    "latex_symbols = {\n",
    "    \"product_dt\": \"\\col{product_dt}{*}\",\n",
    "    \"product_rf\": \"\\col{product_dt}{*}\",\n",
    "    \"sklearn_dt\": \"\\col{euclidean_dt}{†}\",\n",
    "    \"sklearn_rf\": \"\\col{euclidean_dt}{†}\",\n",
    "    \"tangent_dt\": \"\\col{tangent_dt}{‡}\",\n",
    "    \"tangent_rf\": \"\\col{tangent_dt}{‡}\",\n",
    "    \"knn\": \"\\col{knn}{§}\",\n",
    "    \"ps_perceptron\": \"\\col{perceptron}{¶}\",\n",
    "}\n",
    "\n",
    "\n",
    "# Function to format a LaTeX cell with mean ± SE and symbols\n",
    "def format_latex_cell(mean, se, bold=False, underline=False, symbols=None, regression=False):\n",
    "    if regression:\n",
    "        # formatted = f\"{mean:.3f}\".lstrip(\"0\") + \" \\scriptsize ± \" + f\"{se:.3f}\".lstrip(\"0\")\n",
    "        formatted = f\"{mean:.3f}\".lstrip(\"0\") + \" ± \" + f\"{se:.3f}\".lstrip(\"0\")\n",
    "    else:\n",
    "        # formatted = f\"{mean * 100:.1f}\".lstrip(\"0\") + \"\\scriptsize ± \" + f\"{se * 100:.1f}\".lstrip(\"0\")\n",
    "        formatted = f\"{mean * 100:.1f}\".lstrip(\"0\") + \" ± \" + f\"{se * 100:.1f}\".lstrip(\"0\")\n",
    "    if bold:\n",
    "        formatted = f\"\\\\textbf{{{formatted}}}\"\n",
    "    if underline:\n",
    "        formatted = f\"\\\\underline{{{formatted}}}\"\n",
    "    if symbols:\n",
    "        # Remove duplicates\n",
    "        symbols = sorted(list(set(symbols)))\n",
    "        formatted += f\"\\\\textsuperscript{{{''.join(symbols)}}}\"\n",
    "    return formatted\n",
    "\n",
    "\n",
    "# Function to run Wilcoxon test between predictors within group1 and within group2\n",
    "def run_wilcoxon_tests(group1_vals, group2_vals, regression=False):\n",
    "    symbols = {key: [] for key in group1 + group2}\n",
    "\n",
    "    # Compare all pairs within group1\n",
    "    for i, pred1 in enumerate(group1):\n",
    "        for pred2 in group1[i + 1 :]:\n",
    "            if group1_vals[pred1].isnull().all() or group1_vals[pred2].isnull().all():\n",
    "                continue\n",
    "            elif (not regression and group1_vals[pred1].mean() > group1_vals[pred2].mean()) or (\n",
    "                regression and group1_vals[pred1].mean() < group1_vals[pred2].mean()\n",
    "            ):\n",
    "                stat, p_value = wilcoxon(group1_vals[pred1], group1_vals[pred2])\n",
    "                if p_value < P_VAL / ADJUSTMENT:\n",
    "                    symbols[pred1].append(latex_symbols[pred2])\n",
    "                    symbols[pred2].append(latex_symbols[pred1])\n",
    "\n",
    "    # Compare all pairs within group2\n",
    "    for i, pred1 in enumerate(group2):\n",
    "        for pred2 in group2[i + 1 :]:\n",
    "            if group2_vals[pred1].isnull().all() or group2_vals[pred2].isnull().all():\n",
    "                continue\n",
    "            # elif group2_vals[pred1].mean() > group2_vals[pred2].mean():\n",
    "            elif (not regression and group2_vals[pred1].mean() > group2_vals[pred2].mean()) or (\n",
    "                regression and group2_vals[pred1].mean() < group2_vals[pred2].mean()\n",
    "            ):\n",
    "                stat, p_value = wilcoxon(group2_vals[pred1], group2_vals[pred2])\n",
    "                if p_value < P_VAL / ADJUSTMENT:\n",
    "                    symbols[pred1].append(latex_symbols[pred2])\n",
    "                    symbols[pred2].append(latex_symbols[pred1])\n",
    "\n",
    "    return symbols\n",
    "\n",
    "\n",
    "# Group by task and dataset\n",
    "# grouped = all_data.groupby([\"table\", \"task\", \"dataset\", \"signature\"])\n",
    "grouped = all_data.groupby([\"table\", \"dataset\", \"task\", \"signature\"])\n",
    "\n",
    "# Iterate through each group and process data\n",
    "results = {}\n",
    "# for (table, task, dataset, signature), group in grouped:\n",
    "for (table, dataset, task, signature), group in grouped:\n",
    "    group_results = {}\n",
    "    n_samples = {}\n",
    "\n",
    "    # Calculate mean and SE for each predictor\n",
    "    for pred in group1 + group2:\n",
    "        mean, se, n = mean_se(group[pred])\n",
    "        group_results[pred] = {\"mean\": mean, \"se\": se}\n",
    "        n_samples[pred] = n\n",
    "\n",
    "    # Run Wilcoxon tests and gather symbols\n",
    "    symbols = run_wilcoxon_tests(group[group1], group[group2], regression=(task == \"R\"))\n",
    "\n",
    "    # Bold the best predictor\n",
    "    if task == \"R\":\n",
    "        best_pred = min(group_results, key=lambda x: group_results[x][\"mean\"])\n",
    "        second_best_pred = sorted(group_results, key=lambda x: group_results[x][\"mean\"])[1]\n",
    "    else:  # LP and C\n",
    "        best_pred = max(group_results, key=lambda x: group_results[x][\"mean\"])\n",
    "        second_best_pred = sorted(group_results, key=lambda x: group_results[x][\"mean\"])[-2]\n",
    "\n",
    "    # Format the results for LaTeX\n",
    "    # latex_table = {\"$n$\": max(n_samples.values())}\n",
    "    latex_table = {}\n",
    "    for pred in group1 + group2:\n",
    "        bold = pred == best_pred\n",
    "        underline = pred == second_best_pred\n",
    "        latex_table[pred] = format_latex_cell(\n",
    "            group_results[pred][\"mean\"],\n",
    "            group_results[pred][\"se\"],\n",
    "            bold=bold,\n",
    "            underline=underline,\n",
    "            symbols=symbols.get(pred, []),\n",
    "            regression=(task == \"R\"),\n",
    "        )\n",
    "\n",
    "    # results[(table, task, dataset, signature)] = latex_table\n",
    "    results[(table, dataset, task, signature)] = latex_table\n",
    "\n",
    "# Convert results into a dataframe for display\n",
    "latex_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "latex_df.columns = [\n",
    "    # \"$n$\",\n",
    "    \"\\col{product_dt}{Product DT}\",\n",
    "    \"\\col{euclidean_dt}{Euclidean DT}\",\n",
    "    \"\\col{tangent_dt}{Tangent DT}\",\n",
    "    \"\\col{knn}{$k$-Neighbors}\",\n",
    "    \"\\col{perceptron}{PS Perceptron}\",\n",
    "    \"\\col{product_dt}{Product RF}\",\n",
    "    \"\\col{euclidean_dt}{Euclidean RF}\",\n",
    "    \"\\col{tangent_dt}{Tangent RF}\",\n",
    "]\n",
    "\n",
    "# How I want it sorted\n",
    "sort_order_l1 = [\n",
    "    \"Synthetic (single $K$)\",\n",
    "    \"Synthetic (multi-$K$)\",\n",
    "    # \"Empirical\",\n",
    "    \"Graph embeddings\",\n",
    "    # \"VAE embeddings\",\n",
    "    # \"Graph\",\n",
    "    \"VAE\",\n",
    "    \"Other\",\n",
    "]\n",
    "sort_order_l2 = [\"C\", \"R\", \"LP\"]\n",
    "sort_order_l3 = [\n",
    "    \"Gaussian mixture\",\n",
    "    \"Global temperature\",\n",
    "    \"Neural spiking\",\n",
    "    \"PolBlogs\",\n",
    "    \"CiteSeer\",\n",
    "    \"Cora\",\n",
    "    \"CS PhDs\",\n",
    "    \"Football\",\n",
    "    \"Karate Club\",\n",
    "    \"PolBooks\",\n",
    "    \"AdjNoun\",\n",
    "    \"Blood\",\n",
    "    \"Lymphoma\",\n",
    "    \"CIFAR-100\",\n",
    "    \"MNIST\",\n",
    "    \"Traffic\"\n",
    "]\n",
    "sort_order_l3 = [sig_dict[sig] for sig in sig_dict]\n",
    "\n",
    "# Make dicts\n",
    "sort_dict1 = {name: i for i, name in enumerate(sort_order_l1)}\n",
    "sort_dict2 = {name: i for i, name in enumerate(sort_order_l2)}\n",
    "sort_dict3 = {name: i for i, name in enumerate(sort_order_l3)}\n",
    "\n",
    "latex_df = (\n",
    "    latex_df.assign(\n",
    "        sort1=latex_df.index.get_level_values(0).map(sort_dict1),\n",
    "        # sort2=latex_df.index.get_level_values(1).map(sort_dict2),\n",
    "        sort2=latex_df.index.get_level_values(1).map(sort_dict3),\n",
    "        # sort3=latex_df.index.get_level_values(2).map(sort_dict3),\n",
    "        sort3=latex_df.index.get_level_values(2).map(sort_dict2),\n",
    "    )\n",
    "    .sort_values([\"sort1\", \"sort2\", \"sort3\"])\n",
    "    .drop(columns=[\"sort1\", \"sort2\", \"sort3\"])\n",
    ")\n",
    "\n",
    "# Rotate index labels 1 and 2; add spacing for better alignment\n",
    "c_dict = {\n",
    "    \"Synthetic (single $K$)\": \"-5.5cm\",\n",
    "    \"Synthetic (multi-$K$)\": \"-4.5cm\",\n",
    "    \"Graph embeddings\": \"-3cm\",\n",
    "    \"VAE\": \"-1cm\",\n",
    "    \"Other\": \"-1cm\",\n",
    "}\n",
    "latex_df.index = pd.MultiIndex.from_tuples(\n",
    "    # [(f\"\\\\rotatebox{{-90}}{{{c}}}\", f\"\\\\rotatebox{{-90}}{{{t}}}\", d, s) for c, t, d, s in latex_df.index],\n",
    "    [(f\"\\\\rotatebox{{90}}{{\\\\hspace{{{c_dict[c]}}}{c}}}\", t, d, s) for c, t, d, s in latex_df.index],\n",
    "    # names=[\"\", \"Task\", \"Dataset\", \"Signature\"],\n",
    "    names=[\"\", \"Dataset\", \"Task\", \"Signature\"],\n",
    ")\n",
    "\n",
    "# Drop columns that never win; rearrange to suit our needs\n",
    "# latex_df = latex_df.drop(\n",
    "    # columns=[\"\\col{tangent_dt}{Tangent DT}\", \"\\col{perceptron}{PS Perceptron}\", \"\\col{tangent_dt}{Tangent RF}\"]\n",
    "# )\n",
    "latex_df = latex_df[\n",
    "    [\n",
    "        \"\\col{perceptron}{PS Perceptron}\",\n",
    "        \"\\col{knn}{$k$-Neighbors}\",\n",
    "        \"\\col{euclidean_dt}{Euclidean DT}\",\n",
    "        \"\\col{euclidean_dt}{Euclidean RF}\",\n",
    "        \"\\col{tangent_dt}{Tangent DT}\",\n",
    "        \"\\col{tangent_dt}{Tangent RF}\",\n",
    "        \"\\col{product_dt}{Product DT}\",\n",
    "        \"\\col{product_dt}{Product RF}\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = latex_df.to_latex(\n",
    "    # \"../data/results/latex_table.tex\",\n",
    "    escape=False,\n",
    "    # column_format=\"|>{\\centering\\\\arraybackslash}p{2cm}|>{\\centering\\\\arraybackslash}p{2cm}|p{2cm}|p{2cm}|\",\n",
    "    # column_format=\">{\\centering\\\\arraybackslash}p{2cm}>{\\centering\\\\arraybackslash}p{2cm}p{2cm}p{2cm}rrrrrrrrr\",\n",
    "    # column_format=\">{\\centering\\\\arraybackslash}p{1cm}>{\\centering\\\\arraybackslash}p{1cm}p{1.5cm}p{1.5cm}llllllll\",\n",
    "    header=True,\n",
    ")\n",
    "\n",
    "# Remove all occurrences of \"\\cline{3-9}\"\n",
    "latex = latex.replace(\"\\\\cline{3-9}\", \"\")\n",
    "latex = latex.replace(\"\\\\cline{2-9}\", \"\")\n",
    "\n",
    "# Change top bar\n",
    "my_toprule = \"\"\"\\\\toprule\n",
    "& Dataset & Task & Signature & \\col{knn}{$k$-Neighbors} & \\col{euclidean_dt}{Euclidean \\\\ DT} & \\col{euclidean_dt}{Euclidean RF} & \\col{product_dt}{Product DT} & \\col{product_dt}{Product RF} \\\\\\\\\n",
    "\\midrule\"\"\"\n",
    "latex = latex.split(\"\\n\")\n",
    "latex = [latex[0], my_toprule] + latex[5:-4] + latex[-3:]\n",
    "latex = \"\\n\".join(latex)\n",
    "\n",
    "with open(\"../data/results/latex_table.tex\", \"w\") as f:\n",
    "    f.write(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "knn           22\n",
       "product_rf    16\n",
       "sklearn_dt    10\n",
       "product_dt     3\n",
       "tangent_dt     3\n",
       "sklearn_rf     2\n",
       "tangent_rf     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How often are we the max?\n",
    "\n",
    "all_data.groupby([\"table\", \"task\", \"dataset\", \"signature\"]).mean().idxmax(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_rf    30\n",
      "knn           13\n",
      "sklearn_rf     7\n",
      "sklearn_dt     4\n",
      "product_dt     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "product_rf       16\n",
      "tangent_rf       14\n",
      "sklearn_rf       12\n",
      "knn               6\n",
      "sklearn_dt        6\n",
      "product_dt        1\n",
      "ps_perceptron     1\n",
      "tangent_dt        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "True     52\n",
      "False     5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "alldata_grouped = all_data.groupby([\"table\", \"task\", \"dataset\", \"signature\"]).mean()\n",
    "\n",
    "# Flip the sign for regression tasks\n",
    "alldata_grouped.loc[alldata_grouped.index.get_level_values(\"task\") == \"R\"] = -alldata_grouped.loc[\n",
    "    alldata_grouped.index.get_level_values(\"task\") == \"R\"\n",
    "]\n",
    "\n",
    "# How often are we the max?\n",
    "print(alldata_grouped.idxmax(axis=1).value_counts())\n",
    "print()\n",
    "\n",
    "# What about second best?\n",
    "print(alldata_grouped.apply(lambda x: x.nlargest(2).idxmin(), axis=1).value_counts())\n",
    "print()\n",
    "\n",
    "# How often is at least one of our predictors in the top 2?\n",
    "\n",
    "# Then, apply the check for top 2 scores in those columns\n",
    "result = alldata_grouped.apply(\n",
    "    lambda x: any(col in x.nlargest(2).index for col in [\"product_dt\", \"product_rf\"]), axis=1\n",
    ")\n",
    "# Print the counts of True/False values\n",
    "print(result.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
